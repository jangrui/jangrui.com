---
title: k8s安装spark
date: 2020-12-29 16:22:10
permalink: /pages/8d7faf/
categories:
  - 安装
  - K8s
tags:
  - K8s
---

本次部署spark on k8s集群，基于kubeapps，简单便捷且一步到胃

<!-- more -->

## 安装kubeapps


<a href="https://www.xswsym.online/pages/99e4ba/#%E5%9F%BA%E4%BA%8Ehelm">看这里哟</a>

## 选择spark版本
![](https://cdn.jsdelivr.net/gh/summerking1/image@main/825.png)

## yml配置
::: details
```shell
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName

## Bitnami Spark image version
## ref: https://hub.docker.com/r/bitnami/spark/tags/
##
image:
  registry: docker.io
  repository: bitnami/spark
  tag: 2.4.3-debian-9-r78
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Pull secret for this image
  # pullSecrets:
  #   - myRegistryKeySecretName

## String to partially override spark.fullname template (will maintain the release name)
##
# nameOverride:
## String to fully override spark.fullname template
##
# fullnameOverride:
## Spark Components configuration
##
master:
  ## Spark master specific configuration
  ## Set a custom configuration by using an existing configMap with the configuration file.
  # configurationConfigMap:
  webPort: 8080
  clusterPort: 7077

  ## Set the master daemon memory limit.
  # daemonMemoryLimit:
  ## Use a string to set the config options for in the form "-Dx=y"
  # configOptions:
  ## Set to true if you would like to see extra information on logs
  ## It turns BASH and NAMI debugging in minideb
  ## ref:  https://github.com/bitnami/minideb-extras/#turn-on-bash-debugging
  debug: false

  ## An array to add extra env vars
  ## For example:
  ## extraEnvVars:
  ##  - name: SPARK_DAEMON_JAVA_OPTS
  ##    value: -Dx=y
  # extraEnvVars:
  ## Kubernetes Security Context
  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ##
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: 
  #  limits:
  #    cpu: 200m
  #    memory: 1Gi
  #  requests:
  #    memory: 256Mi
  #    cpu: 250m
  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
  livenessProbe:
    enabled: true
    initialDelaySeconds: 180
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

worker:
  ## Spark worker specific configuration
  ## Set a custom configuration by using an existing configMap with the configuration file.
  # configurationConfigMap:
  webPort: 8081
  ## Set to true to use a custom cluster port instead of a random port.
  # clusterPort:
  ## Set the daemonMemoryLimit as the daemon max memory
  # daemonMemoryLimit:
  ## Set the worker memory limit
  # memoryLimit:
  ## Set the maximun number of cores
  # coreLimit:
  ## Working directory for the application
  # dir:
  ## Options for the JVM as "-Dx=y"
  # javaOptions:
  ## Configuraion options in the form "-Dx=y"
  # configOptions:
  ## Number of spark workers (will be the min number when autoscaling is enabled)
  replicaCount: 2

  autoscaling:
    ## Enable replica autoscaling depending on CPU
    enabled: false
    CpuTargetPercentage: 50
    ## Max number of workers when using autoscaling
    replicasMax: 5

  ## Set to true if you would like to see extra information on logs
  ## It turns BASH and NAMI debugging in minideb
  ## ref:  https://github.com/bitnami/minideb-extras/#turn-on-bash-debugging
  debug: false

  ## An array to add extra env vars
  ## For example:
  ## extraEnvVars:
  ##  - name: SPARK_DAEMON_JAVA_OPTS
  ##    value: -Dx=y
  # extraEnvVars:
  ## Kubernetes Security Context
  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ##
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: 
  #  limits:
  #    cpu: 200m
  #    memory: 1Gi
  #  requests:
  #    memory: 256Mi
  #    cpu: 250m
  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
  livenessProbe:
    enabled: true
    initialDelaySeconds: 180
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

## Security configuration
security:
  ## Name of the secret that contains all the passwords. This is optional, by default random passwords are generated.
  # passwordsSecretName:
  ## RPC configuration
  rpc:
    authenticationEnabled: false
    encryptionEnabled: false

  ## Enables local storage encryption
  storageEncryptionEnabled: false

  ## SSL configuration
  ssl:
    enabled: false
    needClientAuth: false
    protocol: TLSv1.2
  ## Name of the secret that contains the certificates
  ## It should contains two keys called "spark-keystore.jks" and "spark-truststore.jks" with the files in JKS format.
  # certificatesSecretName:
## Service to access the master from the workers and to the WebUI
##
service:
  type: NodePort
  clusterPort: 7077
  webPort: 80
  ## Specify the NodePort value for the LoadBalancer and NodePort service types.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
  ##
  # nodePort:
  ## Use loadBalancerIP to request a specific static IP,
  # loadBalancerIP:
  ## Service annotations done as key:value pairs
  annotations: 

## Ingress controller to access the web UI.
ingress:
  enabled: false

  ## Set this to true in order to add the corresponding annotations for cert-manager
  certManager: false

  ## If certManager is set to true, annotation kubernetes.io/tls-acme: "true" will automatically be set
  annotations: 

  ## The list of hostnames to be covered with this ingress record.
  ## Most likely this will be just one host, but in the event more hosts are needed, this is an array
  hosts:
    - name: spark.local
      path: /
```
:::

**执行后耐心等待即可**
![](https://cdn.jsdelivr.net/gh/summerking1/image@main/828.png)
```shell
[root@master home]# kubectl get pods -n kspark
NAME                         READY   STATUS    RESTARTS   AGE
sturdy-cars-spark-master-0   1/1     Running   0          29m
sturdy-cars-spark-worker-0   1/1     Running   0          29m
sturdy-cars-spark-worker-1   1/1     Running   0          5m6s
```
## 验证
```shell
1. Get the Spark master WebUI URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace kspark -o jsonpath="{.spec.ports[0].nodePort}" services sturdy-cars-spark)
  export NODE_IP=$(kubectl get nodes --namespace kspark -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

2. Submit an application to the cluster:

  To submit an application to the cluster the spark-submit script must be used. That script can be
  obtained at https://github.com/apache/spark/tree/master/bin.

  First, obtain the master IP, to do that the service type must be NodePort or LoadBalancer. Run the following command to obtain the master IP and submit your application:
  $ export MASTER_IP=$(kubectl get services | awk '/sturdy-cars-spark/ { print $3 }')
  $ spark-submit --master spark://$MASTER_IP:7077 --deploy-mode cluster  /path/to/application 1000

** IMPORTANT: When submit an application the --master parameter should be set to the service IP, if not, the application will not resolve the master. **

** Please be patient while the chart is being deployed **
```

1. **访问NodePort**
```shell
[root@master home]# kubectl get svc --namespace kspark
NAME                           TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                       AGE
sturdy-cars-spark-headless     ClusterIP   None          <none>        <none>                        101m
sturdy-cars-spark-master-svc   NodePort    10.99.86.55   <none>        7077:32299/TCP,80:30423/TCP   101m
```

**这里可以看到NodePort指向的是30423**

![](https://cdn.jsdelivr.net/gh/summerking1/image@main/826.png)

2. **进入master启动spark shell**

```shell
[root@master home]# kubectl exec -ti sturdy-cars-spark-master-0 -n kspark /bin/sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
$ ls
LICENSE  NOTICE  R  README.md  RELEASE	bin  conf  data  examples  jars  kubernetes  licenses  logs  python  sbin  tmp	work  yarn
$ ls
LICENSE  NOTICE  R  README.md  RELEASE	bin  conf  data  examples  jars  kubernetes  licenses  logs  python  sbin  tmp	work  yarn
$ cd bin
$ ls
beeline		      find-spark-home	   load-spark-env.sh  pyspark2.cmd     spark-class	 spark-shell	   spark-sql	   spark-submit       sparkR
beeline.cmd	      find-spark-home.cmd  pyspark	      run-example      spark-class.cmd	 spark-shell.cmd   spark-sql.cmd   spark-submit.cmd   sparkR.cmd
docker-image-tool.sh  load-spark-env.cmd   pyspark.cmd	      run-example.cmd  spark-class2.cmd  spark-shell2.cmd  spark-sql2.cmd  spark-submit2.cmd  sparkR2.cmd
$ cd ../sbin
$ ls
slaves.sh	  start-all.sh		     start-mesos-shuffle-service.sh  start-thriftserver.sh   stop-mesos-dispatcher.sh	    stop-slaves.sh
spark-config.sh   start-history-server.sh    start-shuffle-service.sh	     stop-all.sh	     stop-mesos-shuffle-service.sh  stop-thriftserver.sh
spark-daemon.sh   start-master.sh	     start-slave.sh		     stop-history-server.sh  stop-shuffle-service.sh
spark-daemons.sh  start-mesos-dispatcher.sh  start-slaves.sh		     stop-master.sh	     stop-slave.sh
$ pwd
/opt/bitnami/spark/sbin 
$ ./spark-shell --master spark://sturdy-cars-spark-master-0.sturdy-cars-spark-headless.kspark.svc.cluster.local:7077
20/12/29 08:11:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://sturdy-cars-spark-master-0.sturdy-cars-spark-headless.kspark.svc.cluster.local:4040
Spark context available as 'sc' (master = spark://sturdy-cars-spark-master-0.sturdy-cars-spark-headless.kspark.svc.cluster.local:7077, app id = app-20201229081130-0000).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_222)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```

3. **web-UI查看**

![](https://cdn.jsdelivr.net/gh/summerking1/image@main/827.png)

4. **未完待续。。。。**